{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a85ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e32815",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f1507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTokenizer:\n",
    "    def __init__(self):\n",
    "        self.VOCABULARY = {\n",
    "            \"A\": 1,\n",
    "            \"U\": 2,\n",
    "            \"C\": 3,\n",
    "            \"G\": 4,\n",
    "        }\n",
    "        self.INVERTED_VOCABULARY = {self.VOCABULARY[i]: i for i in self.VOCABULARY}\n",
    "\n",
    "\n",
    "    def tokenize(self, sequence: str) -> torch.tensor:\n",
    "        return torch.tensor([self.VOCABULARY[i] for i in sequence], dtype=torch.long)\n",
    "    \n",
    "    def detokenize(self, tokens: torch.tensor) -> str:\n",
    "        return \"\".join([self.INVERTED_VOCABULARY[i] for i in tokens.tolist()])\n",
    "    \n",
    "class StructureTokenizer:\n",
    "    def __init__(self):\n",
    "        self.VOCABULARY = {\n",
    "            \"(\": 1,\n",
    "            \")\": 2,\n",
    "            \"[\": 3,\n",
    "            \"]\": 4,\n",
    "            \"{\": 5,\n",
    "            \"}\": 6,\n",
    "            \".\": 7,\n",
    "        }\n",
    "        self.INVERTED_VOCABULARY = {self.VOCABULARY[i]: i for i in self.VOCABULARY}\n",
    "\n",
    "\n",
    "    def tokenize(self, structure: str) -> torch.tensor:\n",
    "        return torch.tensor([self.VOCABULARY[i] for i in structure], dtype=torch.long)\n",
    "    \n",
    "    def detokenize(self, tokens: torch.tensor) -> str:\n",
    "        return \"\".join([self.INVERTED_VOCABULARY[i] for i in tokens.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08875949",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "instead of predicting sequence it predicts pairwise interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7e74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, path: str, indices: List[int]):\n",
    "        \"\"\"path: path to .csv file with sequences and structures\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = pd.read_csv(path)\n",
    "        self.data = self.data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        self.sequence_tokenizer = SequenceTokenizer()\n",
    "        self.structure_tokenizer = StructureTokenizer()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        sequence = row[\"sequence\"]\n",
    "        tokenized_sequence = self.sequence_tokenizer.tokenize(sequence)\n",
    "        structure = row[\"structure\"]\n",
    "        tokenized_structure = self.structure_tokenizer.tokenize(structure)\n",
    "        \n",
    "        return sequence, tokenized_sequence, structure, tokenized_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde52742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, tokenized_seqs, structures, tokenized_structures = zip(*batch)\n",
    "    max_len = max(len(seq) for seq in tokenized_seqs)\n",
    "\n",
    "    padded_tokenized_sequences = []\n",
    "    padded_tokenized_structures = []\n",
    "\n",
    "    for tokenized_sequence, tokenized_structure in zip(tokenized_seqs, tokenized_structures):\n",
    "        sequence_lenght = len(tokenized_sequence)\n",
    "        padded_seq = torch.cat([\n",
    "            tokenized_sequence,\n",
    "            torch.zeros(max_len - sequence_lenght, dtype=torch.long)\n",
    "        ])\n",
    "        padded_structure = torch.cat([\n",
    "            tokenized_structure,\n",
    "            torch.zeros(max_len - sequence_lenght, dtype=torch.long)\n",
    "        ])\n",
    "        padded_tokenized_sequences.append(padded_seq)\n",
    "        padded_tokenized_structures.append(padded_structure)\n",
    "\n",
    "    padded_tokenized_sequences = torch.stack(padded_tokenized_sequences, dim=0)\n",
    "    padded_tokenized_structures = torch.stack(padded_tokenized_structures, dim=0)\n",
    "\n",
    "    return sequences, padded_tokenized_sequences, structures, padded_tokenized_structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709413b",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dc1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformerRNA(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_dim: int=1000, \n",
    "        num_transformer_layers: int=10, \n",
    "        n_head: int=8, \n",
    "        dropout: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # output (L, H)\n",
    "        self.embedding = nn.Embedding(num_embeddings=5, embedding_dim=hidden_dim, padding_idx=0)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "\n",
    "        self.output_head = nn.Linear(hidden_dim, 8)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        padding_mask = (x == 0)\n",
    "        x = self.embedding(x)\n",
    "        # (L, H)\n",
    "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        # (L, H)\n",
    "        x = self.output_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46bec4",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dce34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNATrainer:\n",
    "    def __init__(self, model: transformerRNA, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        class_weights = torch.tensor([\n",
    "            0.0,   # padding (ignored anyway)\n",
    "            5.0,   # ( \n",
    "            5.0,   # )\n",
    "            5.0,   # [\n",
    "            5.0,   # ]\n",
    "            5.0,   # {\n",
    "            5.0,   # }\n",
    "            1.0,   # .\n",
    "        ], dtype=torch.float).to(device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, weight=class_weights)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "    def train_epoch(self, train_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for sequence, tokenized_sequence, structure, tokenized_structure in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            tokenized_sequence = tokenized_sequence.to(self.device)\n",
    "            tokenized_structure = tokenized_structure.to(self.device)\n",
    "            # print(tokenized_sequence)\n",
    "            # padding_mask = padding_mask.to(self.device)\n",
    "\n",
    "            out_logits = self.model(tokenized_sequence)\n",
    "\n",
    "            # print(out_logits.shape)\n",
    "\n",
    "            loss = self.criterion(out_logits.view(-1, 8), tokenized_structure.view(-1))\n",
    "\n",
    "            # valid_mask = (~padding_mask).unsqueeze(-1) & (~padding_mask).unsqueeze(-2)\n",
    "            # loss = (loss * valid_mask).sum() / valid_mask.sum()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_losss = total_loss / len(train_dataloader)\n",
    "        return avg_losss\n",
    "\n",
    "    # def test_model(self, test_loader)\n",
    "    # TODO: test model with reconstructed structure from interaction matrix\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        test_dataloader: torch.utils.data.DataLoader,\n",
    "        num_epochs: int\n",
    "    ) -> None:\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_loss = self.train_epoch(train_dataloader)\n",
    "            print(f\"Epoch {epoch} current loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7bb4f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93b8e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 11500\n",
    "TEST_SIZE = 1646\n",
    "\n",
    "train_indices = list(range(TRAIN_SIZE))\n",
    "test_indices = list(range(TRAIN_SIZE, TRAIN_SIZE+TEST_SIZE))\n",
    "\n",
    "train_dataset = RNADataset(\"rna_dataset.csv\", train_indices)\n",
    "test_dataset = RNADataset(\"rna_dataset.csv\", test_indices)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=16,          # try 16–64; tune to your VRAM\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1879d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:17<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 current loss: 1.0670220129844707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:15<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 current loss: 1.052749472798492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:14<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 current loss: 1.0513404981648973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:15<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 current loss: 1.0504548284374127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:17<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 current loss: 1.0500512302999536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:15<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 current loss: 1.050414896475589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:15<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 current loss: 1.0501475552192816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:13<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 current loss: 1.0493770580795776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:14<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 current loss: 1.05007279698474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 719/719 [02:14<00:00,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 current loss: 1.0493873958793236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = transformerRNA(hidden_dim=512, num_transformer_layers=8, n_head=8)\n",
    "model = model.to(device)\n",
    "trainer = RNATrainer(model, device)\n",
    "trainer.train(train_dataloader, test_dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3efdf266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 25,225,736\n",
      "Model memory: 96.23 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_memory(model):\n",
    "    \"\"\"Calculate model memory in MB\"\"\"\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "    total_size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return total_size_mb\n",
    "\n",
    "# Check your current model\n",
    "model_memory = get_model_memory(model)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model memory: {model_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50af0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_heatmap(matrix) -> None:\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix = matrix.float().cpu().numpy()\n",
    "    sns.heatmap(matrix)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84])\n",
      "tensor([[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]], device='cuda:0')\n",
      "Predicted structure tokens: ....................................................................................\n",
      "Ground truth structure tokens: (((((((..(((...........))).(((((.......)))))............(((((.......))))))))))))....\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "detokenizer = StructureTokenizer()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, tokenized_sequence, structure, tokenized_structure in test_dataloader:\n",
    "        print(tokenized_structure.shape)\n",
    "\n",
    "\n",
    "        tokenized_structure = tokenized_structure.view(-1)\n",
    "        tokenized_sequence = tokenized_sequence.to(device)\n",
    "        out_logits = model(tokenized_sequence)\n",
    "        out_probs = torch.softmax(out_logits, dim=-1)\n",
    "        predicted_structure = torch.argmax(out_probs, dim=-1, )\n",
    "        print(predicted_structure)\n",
    "        print(\"Predicted structure tokens:\", detokenizer.detokenize(predicted_structure[0]))\n",
    "        print(\"Ground truth structure tokens:\", structure[0])\n",
    "\n",
    "        \n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44f6f122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 4, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.sequence_tokenizer.tokenize(\"AUGC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570226c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
