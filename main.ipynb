{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a85ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08875949",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "instead of predicting sequence it predicts pairwise interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7e74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, path: str, indices: List[int]):\n",
    "        \"\"\"path: path to .csv file with sequences and structures\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = pd.read_csv(path)\n",
    "        self.data = self.data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        self.VOCABULARY = {\n",
    "            \"A\": 1,\n",
    "            \"U\": 2,\n",
    "            \"C\": 3,\n",
    "            \"G\": 4,\n",
    "        }\n",
    "\n",
    "        self.pad_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def create_interaction_matrix(self, structure: str) -> torch.tensor:\n",
    "        stack = [[], [], []]\n",
    "        matrix = torch.zeros(len(structure), len(structure), dtype=torch.bfloat16)\n",
    "\n",
    "        for i in range(len(structure)):\n",
    "            match structure[i]:\n",
    "                case \"(\":\n",
    "                    stack[0].append(i)\n",
    "                case \")\":\n",
    "                    matrix[stack[0].pop(), i] = 1\n",
    "                case \"[\":\n",
    "                    stack[1].append(i)\n",
    "                case \"]\":\n",
    "                    matrix[stack[1].pop(), i] = 1\n",
    "                case \"{\":\n",
    "                    stack[2].append(i)\n",
    "                case \"}\":\n",
    "                    matrix[stack[2].pop(), i] = 1\n",
    "                case \".\":\n",
    "                    continue\n",
    "        return matrix\n",
    "    \n",
    "    def tokenize(self, sequence: str) -> torch.tensor:\n",
    "        return torch.tensor([self.VOCABULARY[i] for i in sequence], dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        sequence = row[\"sequence\"]\n",
    "        tokenized_sequence = self.tokenize(sequence)\n",
    "        structure = row[\"structure\"]\n",
    "        pairwise_interaction_matrix = self.create_interaction_matrix(structure)\n",
    "        \n",
    "        return sequence, tokenized_sequence, structure, pairwise_interaction_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709413b",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dc1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformerRNA(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_dim: int=1000, \n",
    "        num_transformer_layers: int=10, \n",
    "        vocab_size: int = 5, \n",
    "        n_head: int=8, \n",
    "        dropout: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # output (L, H)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # output (L, H)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer, \n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        # (H, H)\n",
    "        self.pairwise = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # (L, H)\n",
    "        x = self.encoder(x)\n",
    "        # (L, H)\n",
    "        xW = self.pairwise(x)\n",
    "        # (L, L)\n",
    "        scores = torch.matmul(xW, x.transpose(-2, -1))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46bec4",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dce34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNATrainer:\n",
    "    def __init__(self, model: transformerRNA, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "    def train_epoch(self, train_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for sequence, tokenized_seq, structure, interaction_matrix in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            tokenized_seq = tokenized_seq.to(self.device)\n",
    "            interaction_matrix = interaction_matrix.to(self.device)\n",
    "\n",
    "            out_label = self.model(tokenized_seq)\n",
    "\n",
    "            loss = self.criterion(out_label, interaction_matrix)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_losss = total_loss / len(train_dataloader)\n",
    "        return avg_losss\n",
    "\n",
    "    # def test_model(self, test_loader)\n",
    "    # TODO: test model with reconstructed structure from interaction matrix\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        test_dataloader: torch.utils.data.DataLoader,\n",
    "        num_epochs: int\n",
    "    ) -> None:\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_loss = self.train_epoch(train_dataloader)\n",
    "            print(f\"Epoch {epoch} current loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7bb4f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b8e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 11500\n",
    "TEST_SIZE = 1646\n",
    "\n",
    "train_indices = list(range(TRAIN_SIZE))\n",
    "test_indices = list(range(TRAIN_SIZE, TRAIN_SIZE+TEST_SIZE))\n",
    "\n",
    "train_dataset = RNADataset(\"rna_dataset.csv\", train_indices)\n",
    "test_dataset = RNADataset(\"rna_dataset.csv\", test_indices)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=1,          # try 16–64; tune to your VRAM\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c1879d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 5312/11500 [00:17<00:19, 310.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model = model.to(device)\n\u001b[32m      3\u001b[39m trainer = RNATrainer(model, device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mRNATrainer.train\u001b[39m\u001b[34m(self, train_dataloader, test_dataloader, num_epochs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m, \n\u001b[32m     36\u001b[39m     train_dataloader: torch.utils.data.DataLoader, \n\u001b[32m     37\u001b[39m     test_dataloader: torch.utils.data.DataLoader,\n\u001b[32m     38\u001b[39m     num_epochs: \u001b[38;5;28mint\u001b[39m\n\u001b[32m     39\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         avg_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m current loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mRNATrainer.train_epoch\u001b[39m\u001b[34m(self, train_dataloader)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m     12\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteraction_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenized_seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_seq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43minteraction_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minteraction_matrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:728\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[32m    731\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/autograd/profiler.py:801\u001b[39m, in \u001b[36mrecord_function.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch._C.DisableTorchFunctionSubclass():\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_record_function_exit\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    803\u001b[39m     torch.ops.profiler._record_function_exit(record)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/_ops.py:1069\u001b[39m, in \u001b[36mTorchBindOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1068\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_must_dispatch_in_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1070\u001b[39m         \u001b[38;5;66;03m# When any inputs are FakeScriptObject, we need to\u001b[39;00m\n\u001b[32m   1071\u001b[39m         \u001b[38;5;66;03m# skip c++ dispatcher and dispatch in python through _get_dispatch of python_dispatcher\u001b[39;00m\n\u001b[32m   1072\u001b[39m         \u001b[38;5;66;03m# because C++ dispatcher will check the schema and cannot recognize FakeScriptObject.\u001b[39;00m\n\u001b[32m   1073\u001b[39m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1074\u001b[39m         \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[32m   1075\u001b[39m         \u001b[38;5;66;03m# 1. We only register the torchbind op temporarily as effectful op because we only want\u001b[39;00m\n\u001b[32m   1076\u001b[39m         \u001b[38;5;66;03m#    the effect token functionalization logic to be applied during tracing. Otherwise, the behavior\u001b[39;00m\n\u001b[32m   1077\u001b[39m         \u001b[38;5;66;03m#    of the eagerly executing the op might change after tracing.\u001b[39;00m\n\u001b[32m   1078\u001b[39m         \u001b[38;5;66;03m# 2. We don't want to register the op as effectful for all torchbind ops in ctor because this might\u001b[39;00m\n\u001b[32m   1079\u001b[39m         \u001b[38;5;66;03m#    cause unexpected behavior for some autograd.profiler ops e.g. profiler._record_function_exit._RecordFunction.\u001b[39;00m\n\u001b[32m   1080\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._register_as_effectful_op_temporarily():\n\u001b[32m   1081\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dispatch_in_python(\n\u001b[32m   1082\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fallthrough_keys(), *args, **kwargs\n\u001b[32m   1083\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/_ops.py:1129\u001b[39m, in \u001b[36m_must_dispatch_in_python\u001b[39m\u001b[34m(args, kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_must_dispatch_in_python\u001b[39m(args, kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpytree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_any\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfake_class_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFakeScriptObject\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/utils/_pytree.py:1636\u001b[39m, in \u001b[36mtree_any\u001b[39m\u001b[34m(pred, tree, is_leaf)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtree_any\u001b[39m(\n\u001b[32m   1631\u001b[39m     pred: Callable[[Any], \u001b[38;5;28mbool\u001b[39m],\n\u001b[32m   1632\u001b[39m     tree: PyTree,\n\u001b[32m   1633\u001b[39m     is_leaf: Optional[Callable[[PyTree], \u001b[38;5;28mbool\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1634\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   1635\u001b[39m     flat_args = tree_iter(tree, is_leaf=is_leaf)\n\u001b[32m-> \u001b[39m\u001b[32m1636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/utils/_pytree.py:1307\u001b[39m, in \u001b[36mtree_iter\u001b[39m\u001b[34m(tree, is_leaf)\u001b[39m\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtree_iter\u001b[39m(\n\u001b[32m   1303\u001b[39m     tree: PyTree,\n\u001b[32m   1304\u001b[39m     is_leaf: Optional[Callable[[PyTree], \u001b[38;5;28mbool\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1305\u001b[39m ) -> Iterable[Any]:\n\u001b[32m   1306\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get an iterator over the leaves of a pytree.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtree_is_leaf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m tree\n\u001b[32m   1309\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/utils/_pytree.py:1041\u001b[39m, in \u001b[36mtree_is_leaf\u001b[39m\u001b[34m(tree, is_leaf)\u001b[39m\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_leaf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_leaf(tree):\n\u001b[32m   1040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_node_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_NODES\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/utils/_pytree.py:1014\u001b[39m, in \u001b[36m_get_node_type\u001b[39m\u001b[34m(tree)\u001b[39m\n\u001b[32m   1009\u001b[39m node_type = \u001b[38;5;28mtype\u001b[39m(tree)\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# All namedtuple types are implicitly registered as pytree nodes.\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# XXX: Other parts of the codebase expect namedtuple types always return\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m#      `namedtuple` instead of the actual namedtuple type. Even if the type\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m#      is explicitly registered.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_namedtuple_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_type\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1015\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m namedtuple\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/utils/_pytree.py:679\u001b[39m, in \u001b[36mis_namedtuple_class\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_namedtuple_class\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    676\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return whether the class is a subclass of namedtuple.\"\"\"\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    678\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_fields\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;28mtuple\u001b[39m)\n\u001b[32m    681\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mtype\u001b[39m(field) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._fields)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    682\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_make\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    683\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_asdict\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    684\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = transformerRNA(hidden_dim=100, num_transformer_layers=2, n_head=2)\n",
    "model = model.to(device)\n",
    "trainer = RNATrainer(model, device)\n",
    "trainer.train(train_dataloader, test_dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502094b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
