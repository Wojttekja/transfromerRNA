{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a85ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e32815",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f1507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTokenizer:\n",
    "    def __init__(self):\n",
    "        self.VOCABULARY = {\n",
    "            \"A\": 1,\n",
    "            \"U\": 2,\n",
    "            \"C\": 3,\n",
    "            \"G\": 4,\n",
    "        }\n",
    "        self.INVERTED_VOCABULARY = {self.VOCABULARY[i]: i for i in self.VOCABULARY}\n",
    "\n",
    "\n",
    "    def tokenize(self, sequence: str) -> torch.tensor:\n",
    "        return torch.tensor([self.VOCABULARY[i] for i in sequence], dtype=torch.long)\n",
    "    \n",
    "    def detokenize(self, tokens: torch.tensor) -> str:\n",
    "        return \"\".join([self.INVERTED_VOCABULARY[i] for i in tokens.tolist()])\n",
    "    \n",
    "class StructureTokenizer:\n",
    "    def __init__(self):\n",
    "        self.VOCABULARY = {\n",
    "            \"(\": 1,\n",
    "            \")\": 2,\n",
    "            \"[\": 3,\n",
    "            \"]\": 4,\n",
    "            \"{\": 5,\n",
    "            \"}\": 6,\n",
    "            \".\": 7,\n",
    "        }\n",
    "        self.INVERTED_VOCABULARY = {self.VOCABULARY[i]: i for i in self.VOCABULARY}\n",
    "\n",
    "\n",
    "    def tokenize(self, structure: str) -> torch.tensor:\n",
    "        return torch.tensor([self.VOCABULARY[i] for i in structure], dtype=torch.long)\n",
    "    \n",
    "    def detokenize(self, tokens: torch.tensor) -> str:\n",
    "        return \"\".join([self.INVERTED_VOCABULARY[i] for i in tokens.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08875949",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "instead of predicting sequence it predicts pairwise interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7e74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, path: str, indices: List[int]):\n",
    "        \"\"\"path: path to .csv file with sequences and structures\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = pd.read_csv(path)\n",
    "        self.data = self.data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        self.sequence_tokenizer = SequenceTokenizer()\n",
    "        self.structure_tokenizer = StructureTokenizer()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        sequence = row[\"sequence\"]\n",
    "        tokenized_sequence = self.sequence_tokenizer.tokenize(sequence)\n",
    "        structure = row[\"structure\"]\n",
    "        tokenized_structure = self.structure_tokenizer.tokenize(structure)\n",
    "        \n",
    "        return sequence, tokenized_sequence, structure, tokenized_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde52742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, tokenized_seqs, structures, tokenized_structures = zip(*batch)\n",
    "    max_len = max(len(seq) for seq in tokenized_seqs)\n",
    "\n",
    "    padded_tokenized_sequences = []\n",
    "    padded_tokenized_structures = []\n",
    "\n",
    "    for tokenized_sequence, tokenized_structure in zip(tokenized_seqs, tokenized_structures):\n",
    "        sequence_lenght = len(tokenized_sequence)\n",
    "        padded_seq = torch.cat([\n",
    "            tokenized_sequence,\n",
    "            torch.zeros(max_len - sequence_lenght, dtype=torch.long)\n",
    "        ])\n",
    "        padded_structure = torch.cat([\n",
    "            tokenized_structure,\n",
    "            torch.zeros(max_len - sequence_lenght, dtype=torch.long)\n",
    "        ])\n",
    "        padded_tokenized_sequences.append(padded_seq)\n",
    "        padded_tokenized_structures.append(padded_structure)\n",
    "\n",
    "    padded_tokenized_sequences = torch.stack(padded_tokenized_sequences, dim=0)\n",
    "    padded_tokenized_structures = torch.stack(padded_tokenized_structures, dim=0)\n",
    "\n",
    "    return sequences, padded_tokenized_sequences, structures, padded_tokenized_structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709413b",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dc1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformerRNA(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_dim: int=1000, \n",
    "        num_transformer_layers: int=10, \n",
    "        n_head: int=8, \n",
    "        dropout: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # output (L, H)\n",
    "        self.embedding = nn.Embedding(num_embeddings=5, embedding_dim=hidden_dim, padding_idx=0)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "\n",
    "        self.output_head = nn.Linear(hidden_dim, 8)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        padding_mask = (x == 0)\n",
    "        x = self.embedding(x)\n",
    "        # (L, H)\n",
    "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        # (L, H)\n",
    "        x = self.output_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46bec4",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNATrainer:\n",
    "    def __init__(self, model: transformerRNA, device: torch.device, balance_loss_weight: float=0.1):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        class_weights = torch.tensor([\n",
    "            0.0,   # padding (ignored anyway)\n",
    "            2.6,   # ( \n",
    "            2.6,   # )\n",
    "            98.8,   # [\n",
    "            98.8,   # ]\n",
    "            8064.5,   # {\n",
    "            8064.5,   # }\n",
    "            1.07,   # .\n",
    "        ], dtype=torch.float).to(device)\n",
    "\n",
    "        self.balance_loss_weight = balance_loss_weight\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, weight=class_weights)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "        self.structure_tokenizer = StructureTokenizer()\n",
    "\n",
    "\n",
    "    def compute_bracket_balance(self, predicted_structure) -> torch.Tensor:\n",
    "        batch_size = predicted_structure.shape[0]\n",
    "        balance_loss = 0.0\n",
    "\n",
    "        for i in range(int(batch_size)):\n",
    "            open_round = (predicted_structure[i] == 1).sum().float()   # (\n",
    "            close_round = (predicted_structure[i] == 2).sum().float()  # )\n",
    "            open_square = (predicted_structure[i] == 3).sum().float()  # [\n",
    "            close_square = (predicted_structure[i] == 4).sum().float() # ]\n",
    "            open_curly = (predicted_structure[i] == 5).sum().float()   # {\n",
    "            close_curly = (predicted_structure[i] == 6).sum().float()  # }\n",
    "\n",
    "            balance_loss += torch.abs(open_round - close_round)\n",
    "            balance_loss += torch.abs(open_square - close_square)\n",
    "            balance_loss += torch.abs(open_curly - close_curly)\n",
    "\n",
    "        return balance_loss / batch_size\n",
    "\n",
    "\n",
    "    def train_epoch(self, train_dataloader: torch.utils.data.DataLoader) -> Tuple[float, float, float]:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_bracket_balance_loss = 0\n",
    "\n",
    "        for sequence, tokenized_sequence, structure, tokenized_structure in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            tokenized_sequence = tokenized_sequence.to(self.device)\n",
    "            tokenized_structure = tokenized_structure.to(self.device)\n",
    "            # print(tokenized_sequence)\n",
    "            # padding_mask = padding_mask.to(self.device)\n",
    "\n",
    "            out_logits = self.model(tokenized_sequence)\n",
    "\n",
    "            # print(out_logits.shape)\n",
    "\n",
    "            cross_entropy_loss = self.criterion(out_logits.view(-1, 8), tokenized_structure.view(-1))\n",
    "            total_ce_loss += cross_entropy_loss.item()\n",
    "\n",
    "            # bracket balance loss\n",
    "            predicted_tokens = torch.argmax(out_logits, dim=-1)\n",
    "            balance_loss = self.compute_bracket_balance(predicted_tokens)\n",
    "            total_bracket_balance_loss += balance_loss.item()\n",
    "\n",
    "            \n",
    "            # valid_mask = (~padding_mask).unsqueeze(-1) & (~padding_mask).unsqueeze(-2)\n",
    "            loss = cross_entropy_loss + self.balance_loss_weight * balance_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return float(total_loss), float(total_ce_loss), float(total_bracket_balance_loss)\n",
    "\n",
    "    def test_model(self, test_dataloader):\n",
    "        self.model.eval()\n",
    "\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (sequence, tokenized_sequence, structure, tokenized_structure) in enumerate(test_dataloader):\n",
    "                tokenized_sequence = tokenized_sequence.to(self.device)\n",
    "                tokenized_structure = tokenized_structure.to(self.device)\n",
    "\n",
    "                out_logits = self.model(tokenized_sequence)\n",
    "                \n",
    "                predicted_tokens = torch.argmax(out_logits, dim=-1)\n",
    "\n",
    "                # Calculate accuracy (excluding padding)\n",
    "                non_padding_mask = (tokenized_structure != 0)\n",
    "                correct = (predicted_tokens == tokenized_structure) & non_padding_mask\n",
    "                total_correct += correct.sum().item()\n",
    "                total_tokens += non_padding_mask.sum().item()\n",
    "                \n",
    "                # show example \n",
    "                if idx == 0:\n",
    "                    print(sequence[0])\n",
    "                    print(structure[0])\n",
    "                    non_padded_tokens = predicted_tokens[0][tokenized_structure[0] != 0]\n",
    "                    detokenized_predicted_structure = self.structure_tokenizer.detokenize(non_padded_tokens)\n",
    "                    print(detokenized_predicted_structure)\n",
    "                    print(len(sequence[0]), len(structure[0]), len(detokenized_predicted_structure))\n",
    "\n",
    "        accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "        return accuracy, total_correct, total_tokens\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        test_dataloader: torch.utils.data.DataLoader,\n",
    "        num_epochs: int\n",
    "    ) -> None:\n",
    "        best_loss = float(\"inf\")\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_loss, ce_loss, bracket_loss = self.train_epoch(train_dataloader)\n",
    "            print(f\"Epoch {epoch} current loss: {avg_loss} cross-entropy loss: {ce_loss} bracket_loss {bracket_loss}\")\n",
    "            \n",
    "            accuracy, total_correct, total_tokens = self.test_model(test_dataloader)\n",
    "            print(f\"\\nTest Accuracy: {(accuracy*100):.4f}% ({total_correct}/{total_tokens})\")\n",
    "\n",
    "            best_loss = min(best_loss, avg_loss)\n",
    "        print(\"Best loss\", best_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7bb4f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93b8e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "TRAIN_SIZE = 12149\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "all_indexes = list(range(TRAIN_SIZE+TEST_SIZE))\n",
    "test_indices = random.choices(all_indexes, k=TEST_SIZE)\n",
    "train_indices = [i for i in all_indexes if i not in test_indices]\n",
    "\n",
    "train_dataset = RNADataset(\"rna_dataset.csv\", train_indices)\n",
    "test_dataset = RNADataset(\"rna_dataset.csv\", test_indices)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=64,          # try 16–64; tune to your VRAM\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=100,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cb5ac",
   "metadata": {},
   "source": [
    "# Train 2 example models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1879d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plague/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 191/191 [00:06<00:00, 30.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 current loss: 1669.0869679450989 cross-entropy loss: 369.7909541130066 bracket_loss 64964.80209350586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = transformerRNA(hidden_dim=256, num_transformer_layers=4, n_head=4)\n",
    "model = model.to(device)\n",
    "trainer = RNATrainer(model, device, balance_loss_weight=0.02)\n",
    "trainer.train(train_dataloader, test_dataloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformerRNA(hidden_dim=384, num_transformer_layers=6, n_head=1)\n",
    "model = model.to(device)\n",
    "trainer = RNATrainer(model, device, balance_loss_weight=0.02)\n",
    "trainer.train(train_dataloader, test_dataloader, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
