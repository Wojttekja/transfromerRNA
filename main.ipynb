{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08875949",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "instead of predicting sequence it predicts pairwise interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"path: path to .csv file with sequences and structures\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = pd.read_csv(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def create_interaction_matrix(structure: str) -> torch.tensor:\n",
    "        stack = [[], [], []]\n",
    "        matrix = torch.zeros(len(structure), len(structure), dtype=torch.bfloat16)\n",
    "\n",
    "        for i in range(len(structure)):\n",
    "            match structure[i]:\n",
    "                case \"(\":\n",
    "                    stack[0].append(i)\n",
    "                case \")\":\n",
    "                    matrix[stack[0].pop(), i] = 1\n",
    "                case \"[\":\n",
    "                    stack[1].append(i)\n",
    "                case \"]\":\n",
    "                    matrix[stack[1].pop(), i] = 1\n",
    "                case \"{\":\n",
    "                    stack[2].append(i)\n",
    "                case \"}\":\n",
    "                    matrix[stack[2].pop(), i] = 1\n",
    "                case \".\":\n",
    "                    continue\n",
    "        return matrix\n",
    "    \n",
    "    def tokenize(sequence: str) -> torch.tensor:\n",
    "        VOCABULARY = {\n",
    "            \"A\": torch.int8(0),\n",
    "            \"U\": torch.int8(1),\n",
    "            \"C\": torch.int8(2),\n",
    "            \"G\": torch.int8(3),\n",
    "        }\n",
    "\n",
    "        return torch.tensor([VOCABULARY[i] for i in sequence])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        sequence = row[\"sequence\"]\n",
    "        tokenized_sequence = self.tokenize(sequence)\n",
    "        structure = row[\"structure\"]\n",
    "        matrix = self.create_interaction_matrix(structure)\n",
    "        \n",
    "        return tokenized_sequence, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709413b",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1dc1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformerRNA(nn.Module):\n",
    "    def __init__(self, hidden_dim: int=1000, num_transformer_layers: int=10, vocab_size: int = 4, n_head: int=8, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # output (L, H)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # output (L, H)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer, \n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        # (H, H)\n",
    "        self.pairwise = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # (L, H)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # (L, H)\n",
    "        xW = self.pairwise(x)\n",
    "\n",
    "        # (L, L)\n",
    "        scores = torch.matmul(xW, x.transpose(-2, -1))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7bb4f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8e352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
